{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Fake\n",
    "This is a dataset that has real and computer generated images and the task is to be able to differenciate between these images\n",
    "\n",
    "## Normal CNN\n",
    "The normal CNN model failed to differenciate between the CNN faile to differenctiate between them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "from tqdm.auto import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResConvBlock(nn.Module):\n",
    "    def __init__(self, filter, kernel=3):\n",
    "        super(ResConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(filter, filter, kernel)\n",
    "        self.conv2 = nn.Conv2d(filter, filter, kernel)\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.norm = nn.BatchNorm2d(filter)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x\n",
    "        x = self.lrelu(self.norm(self.pad(self.conv1(x))))\n",
    "        x = self.lrelu(self.norm(self.pad(self.conv2(x)))+s)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBottleNeck(nn.Module):\n",
    "    def __init__(self, infilter, outfilter, kernel=3):\n",
    "        super(ResBottleNeck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(infilter, outfilter, kernel)\n",
    "        self.conv2 = nn.Conv2d(outfilter, outfilter, kernel)\n",
    "        self.conv = nn.Conv2d(infilter, outfilter, 1)\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.norm = nn.BatchNorm2d(outfilter)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x\n",
    "        x = self.pool(self.lrelu(self.norm(self.pad(self.conv1(x)))))\n",
    "        s = self.pool(self.lrelu(self.norm(self.conv(s))))\n",
    "        x = self.lrelu(self.norm(self.pad(self.conv2(x)))+s)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, infilter, outfilter, kernel=3):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv0 = ResConvBlock(infilter)\n",
    "        self.conv1 = ResConvBlock(infilter)\n",
    "        self.conv2 = ResConvBlock(infilter)\n",
    "        self.conv3 = ResConvBlock(infilter)\n",
    "        self.conv4 = ResConvBlock(infilter)\n",
    "        self.conv = ResBottleNeck(infilter, outfilter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.conv4(self.conv3(self.conv2(self.conv1(self.conv0(x))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fake(nn.Module):\n",
    "    def __init__(self, filter, ndim):\n",
    "        super(Fake, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, filter, 7)\n",
    "        self.pad = nn.ZeroPad2d(3)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "        self.norm = nn.BatchNorm2d(filter)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv1 = ResBlock(filter, filter*2, 3)\n",
    "        self.conv2 = ResBlock(filter*2, filter*4, 3)\n",
    "        self.conv3 = ResBlock(filter*4, filter*8, 3)\n",
    "        self.conv4 = ResBlock(filter*8, filter*16, 3)\n",
    "        self.avgpool = nn.AvgPool2d(4, 3)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dense = nn.Linear(4096, ndim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final = nn.Linear(ndim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.lrelu(self.norm(self.pad(self.conv(x)))))\n",
    "        x = self.flat(self.avgpool(self.conv4(self.conv3(self.conv2(self.conv1(x))))))\n",
    "        x = self.sigmoid(self.final(self.relu(self.dense(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 218, 218]           9,472\n",
      "         ZeroPad2d-2         [-1, 64, 224, 224]               0\n",
      "       BatchNorm2d-3         [-1, 64, 224, 224]             128\n",
      "         LeakyReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6         [-1, 64, 110, 110]          36,928\n",
      "         ZeroPad2d-7         [-1, 64, 112, 112]               0\n",
      "       BatchNorm2d-8         [-1, 64, 112, 112]             128\n",
      "         LeakyReLU-9         [-1, 64, 112, 112]               0\n",
      "           Conv2d-10         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-11         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-12         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-13         [-1, 64, 112, 112]               0\n",
      "     ResConvBlock-14         [-1, 64, 112, 112]               0\n",
      "           Conv2d-15         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-16         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-17         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-18         [-1, 64, 112, 112]               0\n",
      "           Conv2d-19         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-20         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-21         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-22         [-1, 64, 112, 112]               0\n",
      "     ResConvBlock-23         [-1, 64, 112, 112]               0\n",
      "           Conv2d-24         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-25         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-26         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-27         [-1, 64, 112, 112]               0\n",
      "           Conv2d-28         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-29         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-30         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-31         [-1, 64, 112, 112]               0\n",
      "     ResConvBlock-32         [-1, 64, 112, 112]               0\n",
      "           Conv2d-33         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-34         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-35         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-36         [-1, 64, 112, 112]               0\n",
      "           Conv2d-37         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-38         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-39         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-40         [-1, 64, 112, 112]               0\n",
      "     ResConvBlock-41         [-1, 64, 112, 112]               0\n",
      "           Conv2d-42         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-43         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-44         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-45         [-1, 64, 112, 112]               0\n",
      "           Conv2d-46         [-1, 64, 110, 110]          36,928\n",
      "        ZeroPad2d-47         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-48         [-1, 64, 112, 112]             128\n",
      "        LeakyReLU-49         [-1, 64, 112, 112]               0\n",
      "     ResConvBlock-50         [-1, 64, 112, 112]               0\n",
      "           Conv2d-51        [-1, 128, 110, 110]          73,856\n",
      "        ZeroPad2d-52        [-1, 128, 112, 112]               0\n",
      "      BatchNorm2d-53        [-1, 128, 112, 112]             256\n",
      "        LeakyReLU-54        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-55          [-1, 128, 56, 56]               0\n",
      "           Conv2d-56        [-1, 128, 112, 112]           8,320\n",
      "      BatchNorm2d-57        [-1, 128, 112, 112]             256\n",
      "        LeakyReLU-58        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-59          [-1, 128, 56, 56]               0\n",
      "           Conv2d-60          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-61          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-62          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-63          [-1, 128, 56, 56]               0\n",
      "    ResBottleNeck-64          [-1, 128, 56, 56]               0\n",
      "         ResBlock-65          [-1, 128, 56, 56]               0\n",
      "           Conv2d-66          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-67          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-68          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-69          [-1, 128, 56, 56]               0\n",
      "           Conv2d-70          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-71          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-72          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-73          [-1, 128, 56, 56]               0\n",
      "     ResConvBlock-74          [-1, 128, 56, 56]               0\n",
      "           Conv2d-75          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-76          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-77          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-78          [-1, 128, 56, 56]               0\n",
      "           Conv2d-79          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-80          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-81          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-82          [-1, 128, 56, 56]               0\n",
      "     ResConvBlock-83          [-1, 128, 56, 56]               0\n",
      "           Conv2d-84          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-85          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-86          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-87          [-1, 128, 56, 56]               0\n",
      "           Conv2d-88          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-89          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-90          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-91          [-1, 128, 56, 56]               0\n",
      "     ResConvBlock-92          [-1, 128, 56, 56]               0\n",
      "           Conv2d-93          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-94          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-95          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-96          [-1, 128, 56, 56]               0\n",
      "           Conv2d-97          [-1, 128, 54, 54]         147,584\n",
      "        ZeroPad2d-98          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-99          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-100          [-1, 128, 56, 56]               0\n",
      "    ResConvBlock-101          [-1, 128, 56, 56]               0\n",
      "          Conv2d-102          [-1, 128, 54, 54]         147,584\n",
      "       ZeroPad2d-103          [-1, 128, 56, 56]               0\n",
      "     BatchNorm2d-104          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-105          [-1, 128, 56, 56]               0\n",
      "          Conv2d-106          [-1, 128, 54, 54]         147,584\n",
      "       ZeroPad2d-107          [-1, 128, 56, 56]               0\n",
      "     BatchNorm2d-108          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-109          [-1, 128, 56, 56]               0\n",
      "    ResConvBlock-110          [-1, 128, 56, 56]               0\n",
      "          Conv2d-111          [-1, 256, 54, 54]         295,168\n",
      "       ZeroPad2d-112          [-1, 256, 56, 56]               0\n",
      "     BatchNorm2d-113          [-1, 256, 56, 56]             512\n",
      "       LeakyReLU-114          [-1, 256, 56, 56]               0\n",
      "       MaxPool2d-115          [-1, 256, 28, 28]               0\n",
      "          Conv2d-116          [-1, 256, 56, 56]          33,024\n",
      "     BatchNorm2d-117          [-1, 256, 56, 56]             512\n",
      "       LeakyReLU-118          [-1, 256, 56, 56]               0\n",
      "       MaxPool2d-119          [-1, 256, 28, 28]               0\n",
      "          Conv2d-120          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-121          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-122          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-123          [-1, 256, 28, 28]               0\n",
      "   ResBottleNeck-124          [-1, 256, 28, 28]               0\n",
      "        ResBlock-125          [-1, 256, 28, 28]               0\n",
      "          Conv2d-126          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-127          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-128          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-129          [-1, 256, 28, 28]               0\n",
      "          Conv2d-130          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-131          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-132          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-133          [-1, 256, 28, 28]               0\n",
      "    ResConvBlock-134          [-1, 256, 28, 28]               0\n",
      "          Conv2d-135          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-136          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-137          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-138          [-1, 256, 28, 28]               0\n",
      "          Conv2d-139          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-140          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-141          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-142          [-1, 256, 28, 28]               0\n",
      "    ResConvBlock-143          [-1, 256, 28, 28]               0\n",
      "          Conv2d-144          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-145          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-146          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-147          [-1, 256, 28, 28]               0\n",
      "          Conv2d-148          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-149          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-150          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-151          [-1, 256, 28, 28]               0\n",
      "    ResConvBlock-152          [-1, 256, 28, 28]               0\n",
      "          Conv2d-153          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-154          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-155          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-156          [-1, 256, 28, 28]               0\n",
      "          Conv2d-157          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-158          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-159          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-160          [-1, 256, 28, 28]               0\n",
      "    ResConvBlock-161          [-1, 256, 28, 28]               0\n",
      "          Conv2d-162          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-163          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-164          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-165          [-1, 256, 28, 28]               0\n",
      "          Conv2d-166          [-1, 256, 26, 26]         590,080\n",
      "       ZeroPad2d-167          [-1, 256, 28, 28]               0\n",
      "     BatchNorm2d-168          [-1, 256, 28, 28]             512\n",
      "       LeakyReLU-169          [-1, 256, 28, 28]               0\n",
      "    ResConvBlock-170          [-1, 256, 28, 28]               0\n",
      "          Conv2d-171          [-1, 512, 26, 26]       1,180,160\n",
      "       ZeroPad2d-172          [-1, 512, 28, 28]               0\n",
      "     BatchNorm2d-173          [-1, 512, 28, 28]           1,024\n",
      "       LeakyReLU-174          [-1, 512, 28, 28]               0\n",
      "       MaxPool2d-175          [-1, 512, 14, 14]               0\n",
      "          Conv2d-176          [-1, 512, 28, 28]         131,584\n",
      "     BatchNorm2d-177          [-1, 512, 28, 28]           1,024\n",
      "       LeakyReLU-178          [-1, 512, 28, 28]               0\n",
      "       MaxPool2d-179          [-1, 512, 14, 14]               0\n",
      "          Conv2d-180          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-181          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-182          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-183          [-1, 512, 14, 14]               0\n",
      "   ResBottleNeck-184          [-1, 512, 14, 14]               0\n",
      "        ResBlock-185          [-1, 512, 14, 14]               0\n",
      "          Conv2d-186          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-187          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-188          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-189          [-1, 512, 14, 14]               0\n",
      "          Conv2d-190          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-191          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-192          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-193          [-1, 512, 14, 14]               0\n",
      "    ResConvBlock-194          [-1, 512, 14, 14]               0\n",
      "          Conv2d-195          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-196          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-197          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-198          [-1, 512, 14, 14]               0\n",
      "          Conv2d-199          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-200          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-201          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-202          [-1, 512, 14, 14]               0\n",
      "    ResConvBlock-203          [-1, 512, 14, 14]               0\n",
      "          Conv2d-204          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-205          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-206          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-207          [-1, 512, 14, 14]               0\n",
      "          Conv2d-208          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-209          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-210          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-211          [-1, 512, 14, 14]               0\n",
      "    ResConvBlock-212          [-1, 512, 14, 14]               0\n",
      "          Conv2d-213          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-214          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-215          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-216          [-1, 512, 14, 14]               0\n",
      "          Conv2d-217          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-218          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-219          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-220          [-1, 512, 14, 14]               0\n",
      "    ResConvBlock-221          [-1, 512, 14, 14]               0\n",
      "          Conv2d-222          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-223          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-224          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-225          [-1, 512, 14, 14]               0\n",
      "          Conv2d-226          [-1, 512, 12, 12]       2,359,808\n",
      "       ZeroPad2d-227          [-1, 512, 14, 14]               0\n",
      "     BatchNorm2d-228          [-1, 512, 14, 14]           1,024\n",
      "       LeakyReLU-229          [-1, 512, 14, 14]               0\n",
      "    ResConvBlock-230          [-1, 512, 14, 14]               0\n",
      "          Conv2d-231         [-1, 1024, 12, 12]       4,719,616\n",
      "       ZeroPad2d-232         [-1, 1024, 14, 14]               0\n",
      "     BatchNorm2d-233         [-1, 1024, 14, 14]           2,048\n",
      "       LeakyReLU-234         [-1, 1024, 14, 14]               0\n",
      "       MaxPool2d-235           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-236         [-1, 1024, 14, 14]         525,312\n",
      "     BatchNorm2d-237         [-1, 1024, 14, 14]           2,048\n",
      "       LeakyReLU-238         [-1, 1024, 14, 14]               0\n",
      "       MaxPool2d-239           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-240           [-1, 1024, 5, 5]       9,438,208\n",
      "       ZeroPad2d-241           [-1, 1024, 7, 7]               0\n",
      "     BatchNorm2d-242           [-1, 1024, 7, 7]           2,048\n",
      "       LeakyReLU-243           [-1, 1024, 7, 7]               0\n",
      "   ResBottleNeck-244           [-1, 1024, 7, 7]               0\n",
      "        ResBlock-245           [-1, 1024, 7, 7]               0\n",
      "       AvgPool2d-246           [-1, 1024, 2, 2]               0\n",
      "         Flatten-247                 [-1, 4096]               0\n",
      "          Linear-248                  [-1, 200]         819,400\n",
      "            ReLU-249                  [-1, 200]               0\n",
      "          Linear-250                    [-1, 1]             201\n",
      "         Sigmoid-251                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 51,706,641\n",
      "Trainable params: 51,706,641\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 815.45\n",
      "Params size (MB): 197.25\n",
      "Estimated Total Size (MB): 1013.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Fake(64, 200)\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = r\"C:\\Users\\suyash\\Desktop\\deep fake\\metadata.csv\"\n",
    "dataset = pd.read_csv(CSV)\n",
    "dataset.head()\n",
    "FOLDER = r\"C:\\Users\\suyash\\Desktop\\deep fake\\deepfake\"\n",
    "files = FOLDER+ \"\\\\\" + dataset[\"videoname\"].values\n",
    "dataset.loc[dataset[\"label\"]==\"FAKE\", \"label\"] = 0\n",
    "dataset.loc[dataset[\"label\"]==\"REAL\", \"label\"] = 1\n",
    "vals = dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = []\n",
    "ytrain = []\n",
    "for c, (file, v) in enumerate(zip(files, vals)):\n",
    "    if v == 1 or c%4 == 0:\n",
    "        ytrain.append(v)\n",
    "        file = file[:-3]+\"jpg\"\n",
    "        img = cv2.resize(cv2.imread(file), (224,224))/255.0\n",
    "        xtrain.append(img)\n",
    "        if len(ytrain) >= 5000:\n",
    "            break\n",
    "\n",
    "# converting to rch satatement removed         \n",
    "X = np.array(xtrain, dtype=\"float32\")\n",
    "Y = np.array(ytrain, dtype=\"float32\")\n",
    "np.save(r\"C:\\Users\\suyash\\Desktop\\X.npy\", X)\n",
    "np.save(r\"C:\\Users\\suyash\\Desktop\\Y.npy\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.43529412, 0.44313726, 0.44313726],\n",
       "         [0.43529412, 0.44313726, 0.44313726],\n",
       "         [0.4392157 , 0.44705883, 0.44705883],\n",
       "         ...,\n",
       "         [0.03921569, 0.03137255, 0.05490196],\n",
       "         [0.04313726, 0.03529412, 0.05882353],\n",
       "         [0.04705882, 0.03921569, 0.0627451 ]],\n",
       "\n",
       "        [[0.43529412, 0.44313726, 0.44313726],\n",
       "         [0.43529412, 0.44313726, 0.44313726],\n",
       "         [0.4392157 , 0.44705883, 0.44705883],\n",
       "         ...,\n",
       "         [0.03921569, 0.03137255, 0.05490196],\n",
       "         [0.04313726, 0.03529412, 0.05882353],\n",
       "         [0.04705882, 0.03921569, 0.0627451 ]],\n",
       "\n",
       "        [[0.43529412, 0.44313726, 0.44313726],\n",
       "         [0.43529412, 0.44313726, 0.44313726],\n",
       "         [0.4392157 , 0.44705883, 0.44705883],\n",
       "         ...,\n",
       "         [0.03921569, 0.03137255, 0.05490196],\n",
       "         [0.04313726, 0.03529412, 0.05882353],\n",
       "         [0.04705882, 0.03921569, 0.0627451 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.05490196, 0.01176471, 0.09019608],\n",
       "         [0.05490196, 0.01176471, 0.09019608],\n",
       "         [0.05490196, 0.00784314, 0.09411765],\n",
       "         ...,\n",
       "         [0.08235294, 0.05882353, 0.13333334],\n",
       "         [0.07843138, 0.05490196, 0.12941177],\n",
       "         [0.07450981, 0.05098039, 0.1254902 ]],\n",
       "\n",
       "        [[0.05882353, 0.01568628, 0.09411765],\n",
       "         [0.05490196, 0.01176471, 0.09019608],\n",
       "         [0.05490196, 0.00784314, 0.09411765],\n",
       "         ...,\n",
       "         [0.07843138, 0.05490196, 0.12941177],\n",
       "         [0.07450981, 0.05098039, 0.1254902 ],\n",
       "         [0.07450981, 0.05098039, 0.1254902 ]],\n",
       "\n",
       "        [[0.05882353, 0.01568628, 0.09411765],\n",
       "         [0.05882353, 0.01568628, 0.09411765],\n",
       "         [0.05882353, 0.01176471, 0.09803922],\n",
       "         ...,\n",
       "         [0.07843138, 0.05490196, 0.12941177],\n",
       "         [0.07450981, 0.05098039, 0.1254902 ],\n",
       "         [0.07450981, 0.05098039, 0.1254902 ]]],\n",
       "\n",
       "\n",
       "       [[[0.2784314 , 0.30588236, 0.31764707],\n",
       "         [0.2901961 , 0.31764707, 0.32941177],\n",
       "         [0.29803923, 0.3254902 , 0.3372549 ],\n",
       "         ...,\n",
       "         [0.03529412, 0.02745098, 0.02745098],\n",
       "         [0.03529412, 0.02745098, 0.02745098],\n",
       "         [0.03529412, 0.02745098, 0.02745098]],\n",
       "\n",
       "        [[0.2784314 , 0.30588236, 0.31764707],\n",
       "         [0.2901961 , 0.31764707, 0.32941177],\n",
       "         [0.29803923, 0.3254902 , 0.3372549 ],\n",
       "         ...,\n",
       "         [0.03529412, 0.02745098, 0.02745098],\n",
       "         [0.03529412, 0.02745098, 0.02745098],\n",
       "         [0.03529412, 0.02745098, 0.02745098]],\n",
       "\n",
       "        [[0.28235295, 0.30980393, 0.32156864],\n",
       "         [0.2901961 , 0.31764707, 0.32941177],\n",
       "         [0.29803923, 0.3254902 , 0.3372549 ],\n",
       "         ...,\n",
       "         [0.03529412, 0.02745098, 0.02745098],\n",
       "         [0.03529412, 0.02745098, 0.02745098],\n",
       "         [0.03529412, 0.02745098, 0.02745098]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.17254902, 0.2       , 0.23529412],\n",
       "         [0.1764706 , 0.20392157, 0.23921569],\n",
       "         [0.18431373, 0.21176471, 0.24705882],\n",
       "         ...,\n",
       "         [0.23137255, 0.2901961 , 0.3254902 ],\n",
       "         [0.23137255, 0.2901961 , 0.3254902 ],\n",
       "         [0.23529412, 0.29411766, 0.32941177]],\n",
       "\n",
       "        [[0.17254902, 0.2       , 0.23529412],\n",
       "         [0.1764706 , 0.20392157, 0.23921569],\n",
       "         [0.18431373, 0.21176471, 0.24705882],\n",
       "         ...,\n",
       "         [0.23137255, 0.2901961 , 0.3254902 ],\n",
       "         [0.23529412, 0.29411766, 0.32941177],\n",
       "         [0.23529412, 0.29411766, 0.32941177]],\n",
       "\n",
       "        [[0.17254902, 0.2       , 0.23529412],\n",
       "         [0.1764706 , 0.20392157, 0.23921569],\n",
       "         [0.18431373, 0.21176471, 0.24705882],\n",
       "         ...,\n",
       "         [0.23529412, 0.29411766, 0.32941177],\n",
       "         [0.23529412, 0.29411766, 0.32941177],\n",
       "         [0.23529412, 0.29411766, 0.32941177]]],\n",
       "\n",
       "\n",
       "       [[[0.15686275, 0.1764706 , 0.36862746],\n",
       "         [0.16078432, 0.18431373, 0.36862746],\n",
       "         [0.16078432, 0.18431373, 0.36862746],\n",
       "         ...,\n",
       "         [0.07450981, 0.09411765, 0.1764706 ],\n",
       "         [0.07450981, 0.08627451, 0.16862746],\n",
       "         [0.0627451 , 0.07450981, 0.15686275]],\n",
       "\n",
       "        [[0.15294118, 0.17254902, 0.3647059 ],\n",
       "         [0.15686275, 0.18039216, 0.3647059 ],\n",
       "         [0.16078432, 0.18431373, 0.36862746],\n",
       "         ...,\n",
       "         [0.07843138, 0.09803922, 0.18039216],\n",
       "         [0.07450981, 0.08627451, 0.16862746],\n",
       "         [0.06666667, 0.07843138, 0.16078432]],\n",
       "\n",
       "        [[0.15294118, 0.17254902, 0.3647059 ],\n",
       "         [0.15686275, 0.18039216, 0.3647059 ],\n",
       "         [0.15686275, 0.18039216, 0.3647059 ],\n",
       "         ...,\n",
       "         [0.07843138, 0.09411765, 0.1882353 ],\n",
       "         [0.07843138, 0.09019608, 0.17254902],\n",
       "         [0.07058824, 0.08235294, 0.16470589]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.12156863, 0.17254902, 0.3529412 ],\n",
       "         [0.12156863, 0.17254902, 0.3529412 ],\n",
       "         [0.12156863, 0.17254902, 0.3529412 ],\n",
       "         ...,\n",
       "         [0.        , 0.02352941, 0.07843138],\n",
       "         [0.        , 0.03137255, 0.08235294],\n",
       "         [0.        , 0.03529412, 0.07450981]],\n",
       "\n",
       "        [[0.12941177, 0.18039216, 0.36078432],\n",
       "         [0.12941177, 0.18039216, 0.36078432],\n",
       "         [0.12941177, 0.18039216, 0.36078432],\n",
       "         ...,\n",
       "         [0.        , 0.02352941, 0.07843138],\n",
       "         [0.        , 0.03137255, 0.08235294],\n",
       "         [0.        , 0.03529412, 0.07450981]],\n",
       "\n",
       "        [[0.12941177, 0.18039216, 0.36078432],\n",
       "         [0.12941177, 0.18039216, 0.36078432],\n",
       "         [0.12941177, 0.18039216, 0.36078432],\n",
       "         ...,\n",
       "         [0.        , 0.02352941, 0.07450981],\n",
       "         [0.        , 0.03137255, 0.08235294],\n",
       "         [0.        , 0.03529412, 0.07450981]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.14117648, 0.25490198, 0.3137255 ],\n",
       "         [0.14901961, 0.2627451 , 0.32156864],\n",
       "         [0.16470589, 0.26666668, 0.32156864],\n",
       "         ...,\n",
       "         [0.05882353, 0.07450981, 0.09411765],\n",
       "         [0.0627451 , 0.07450981, 0.09411765],\n",
       "         [0.0627451 , 0.07450981, 0.09411765]],\n",
       "\n",
       "        [[0.14509805, 0.25882354, 0.31764707],\n",
       "         [0.15294118, 0.26666668, 0.3254902 ],\n",
       "         [0.16862746, 0.27058825, 0.3254902 ],\n",
       "         ...,\n",
       "         [0.05882353, 0.07450981, 0.09411765],\n",
       "         [0.0627451 , 0.07450981, 0.09411765],\n",
       "         [0.0627451 , 0.07450981, 0.09411765]],\n",
       "\n",
       "        [[0.15294118, 0.26666668, 0.3254902 ],\n",
       "         [0.15686275, 0.27058825, 0.32941177],\n",
       "         [0.17254902, 0.27450982, 0.32941177],\n",
       "         ...,\n",
       "         [0.05882353, 0.07058824, 0.10196079],\n",
       "         [0.0627451 , 0.07450981, 0.09411765],\n",
       "         [0.0627451 , 0.07450981, 0.09411765]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.05882353, 0.10196079, 0.15686275],\n",
       "         [0.05882353, 0.10196079, 0.15686275],\n",
       "         [0.05882353, 0.10196079, 0.15686275],\n",
       "         ...,\n",
       "         [0.01960784, 0.03529412, 0.03921569],\n",
       "         [0.01568628, 0.03529412, 0.03921569],\n",
       "         [0.01176471, 0.03137255, 0.03529412]],\n",
       "\n",
       "        [[0.05882353, 0.10196079, 0.15686275],\n",
       "         [0.05882353, 0.10196079, 0.15686275],\n",
       "         [0.05882353, 0.10196079, 0.15686275],\n",
       "         ...,\n",
       "         [0.01960784, 0.03529412, 0.03921569],\n",
       "         [0.01568628, 0.03529412, 0.03921569],\n",
       "         [0.01176471, 0.03137255, 0.03529412]],\n",
       "\n",
       "        [[0.05490196, 0.09803922, 0.15294118],\n",
       "         [0.05882353, 0.10196079, 0.15686275],\n",
       "         [0.05882353, 0.10196079, 0.15686275],\n",
       "         ...,\n",
       "         [0.01960784, 0.03529412, 0.03921569],\n",
       "         [0.01568628, 0.03529412, 0.03921569],\n",
       "         [0.01176471, 0.03137255, 0.03529412]]],\n",
       "\n",
       "\n",
       "       [[[0.07058824, 0.08235294, 0.09803922],\n",
       "         [0.06666667, 0.07843138, 0.09411765],\n",
       "         [0.05882353, 0.07058824, 0.09019608],\n",
       "         ...,\n",
       "         [0.61960787, 0.627451  , 0.627451  ],\n",
       "         [0.61960787, 0.627451  , 0.627451  ],\n",
       "         [0.61960787, 0.627451  , 0.627451  ]],\n",
       "\n",
       "        [[0.06666667, 0.07843138, 0.09411765],\n",
       "         [0.06666667, 0.07843138, 0.09411765],\n",
       "         [0.07058824, 0.08235294, 0.10196079],\n",
       "         ...,\n",
       "         [0.61960787, 0.627451  , 0.627451  ],\n",
       "         [0.61960787, 0.627451  , 0.627451  ],\n",
       "         [0.61960787, 0.627451  , 0.627451  ]],\n",
       "\n",
       "        [[0.0627451 , 0.07450981, 0.09019608],\n",
       "         [0.06666667, 0.07843138, 0.09411765],\n",
       "         [0.07450981, 0.08627451, 0.10588235],\n",
       "         ...,\n",
       "         [0.61960787, 0.627451  , 0.627451  ],\n",
       "         [0.61960787, 0.627451  , 0.627451  ],\n",
       "         [0.61960787, 0.627451  , 0.627451  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.24705882, 0.32941177, 0.45490196],\n",
       "         [0.24705882, 0.32941177, 0.45490196],\n",
       "         [0.24313726, 0.3254902 , 0.4509804 ],\n",
       "         ...,\n",
       "         [0.6509804 , 0.67058825, 0.6745098 ],\n",
       "         [0.6509804 , 0.67058825, 0.6745098 ],\n",
       "         [0.654902  , 0.6745098 , 0.6784314 ]],\n",
       "\n",
       "        [[0.25490198, 0.3372549 , 0.4627451 ],\n",
       "         [0.2509804 , 0.33333334, 0.45882353],\n",
       "         [0.24705882, 0.32941177, 0.45490196],\n",
       "         ...,\n",
       "         [0.654902  , 0.6745098 , 0.6784314 ],\n",
       "         [0.654902  , 0.6745098 , 0.6784314 ],\n",
       "         [0.65882355, 0.6784314 , 0.68235296]],\n",
       "\n",
       "        [[0.25490198, 0.3372549 , 0.4627451 ],\n",
       "         [0.25490198, 0.3372549 , 0.4627451 ],\n",
       "         [0.2509804 , 0.33333334, 0.45882353],\n",
       "         ...,\n",
       "         [0.65882355, 0.6784314 , 0.68235296],\n",
       "         [0.65882355, 0.6784314 , 0.68235296],\n",
       "         [0.6627451 , 0.68235296, 0.6862745 ]]],\n",
       "\n",
       "\n",
       "       [[[0.01568628, 0.01960784, 0.03529412],\n",
       "         [0.01568628, 0.01960784, 0.03529412],\n",
       "         [0.01568628, 0.01960784, 0.03529412],\n",
       "         ...,\n",
       "         [0.06666667, 0.10196079, 0.15294118],\n",
       "         [0.06666667, 0.10196079, 0.15294118],\n",
       "         [0.06666667, 0.10196079, 0.15294118]],\n",
       "\n",
       "        [[0.01568628, 0.01960784, 0.03529412],\n",
       "         [0.01568628, 0.01960784, 0.03529412],\n",
       "         [0.01568628, 0.01960784, 0.03529412],\n",
       "         ...,\n",
       "         [0.06666667, 0.10196079, 0.15294118],\n",
       "         [0.06666667, 0.10196079, 0.15294118],\n",
       "         [0.06666667, 0.10196079, 0.15294118]],\n",
       "\n",
       "        [[0.01568628, 0.01960784, 0.03529412],\n",
       "         [0.01568628, 0.01960784, 0.03529412],\n",
       "         [0.01568628, 0.01960784, 0.03529412],\n",
       "         ...,\n",
       "         [0.06666667, 0.10196079, 0.15294118],\n",
       "         [0.07058824, 0.10588235, 0.15686275],\n",
       "         [0.07058824, 0.10588235, 0.15686275]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.01568628, 0.02352941, 0.02745098],\n",
       "         [0.01568628, 0.02352941, 0.02745098],\n",
       "         [0.01568628, 0.02352941, 0.02745098],\n",
       "         ...,\n",
       "         [0.01960784, 0.02352941, 0.03921569],\n",
       "         [0.01960784, 0.02352941, 0.03921569],\n",
       "         [0.01960784, 0.02352941, 0.03921569]],\n",
       "\n",
       "        [[0.01568628, 0.02352941, 0.02745098],\n",
       "         [0.01568628, 0.02352941, 0.02745098],\n",
       "         [0.01568628, 0.02352941, 0.02745098],\n",
       "         ...,\n",
       "         [0.01960784, 0.02352941, 0.03921569],\n",
       "         [0.01960784, 0.02352941, 0.03921569],\n",
       "         [0.01960784, 0.02352941, 0.03921569]],\n",
       "\n",
       "        [[0.01568628, 0.02352941, 0.02745098],\n",
       "         [0.01568628, 0.02352941, 0.02745098],\n",
       "         [0.01568628, 0.02352941, 0.02745098],\n",
       "         ...,\n",
       "         [0.01960784, 0.02352941, 0.03921569],\n",
       "         [0.01960784, 0.02352941, 0.03921569],\n",
       "         [0.01960784, 0.02352941, 0.03921569]]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for y in Y[:4500]:\n",
    "    if y==0:\n",
    "        c=c+1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "decay = 1\n",
    "criterian = nn.BCELoss()\n",
    "batchSize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = DataLoader(X[:4500],batch_size = batchSize)\n",
    "ytrain = DataLoader(Y[:4500], batch_size=batchSize)\n",
    "\n",
    "xtest = DataLoader(X[4500:],batch_size = batchSize)\n",
    "ytest = DataLoader(Y[4500:], batch_size=batchSize)\n",
    "\n",
    "xtrain = np.array([xtr for xtr in xtrain])\n",
    "ytrain = np.array([ytr for ytr in ytrain])\n",
    "xtest = np.array([xtr for xtr in xtest])\n",
    "ytest = np.array([ytr for ytr in ytest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05657c79446e40548d26131509556d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "\tTraining: 0/450 steps || Loss: NaNaN || Progress:   0%|                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8168/824438269.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\tTraining: {c+1}/{steps} steps || Loss: {lss/(c+1):.4f} || Progress\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\suyash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\suyash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = len(xtrain)\n",
    "vsteps = len(xtest)\n",
    "for epoch in range(epochs):\n",
    "    lss = 0\n",
    "    vls = 0\n",
    "    learning_rate = learning_rate/(epoch*decay+1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "    train = trange(steps, desc=f\"\\tTraining: 0/{steps} steps || Loss: NaNaN || Progress\", unit=\"step\", ncols=1000)\n",
    "    for c in train:\n",
    "        xtr = xtrain[c]\n",
    "        ytr = ytrain[c]\n",
    "        ypred = model(xtr.reshape(batchSize, 3, 224, 224))\n",
    "        loss = criterian(ypred, ytr.reshape(-1,1))\n",
    "        lss = lss + loss    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train.set_description(f\"\\tTraining: {c+1}/{steps} steps || Loss: {lss/(c+1):.4f} || Progress\", refresh=True)\n",
    "\n",
    "    test = trange(vsteps, desc=f\"\\tValidation: 0/{vsteps} steps || Loss: NaNaN || Progress:\", unit=\"step\", ncols=1000)\n",
    "    with torch.no_grad():\n",
    "        for c in test:\n",
    "            xtr = xtest[c]\n",
    "            ytr = ytest[c]        \n",
    "            ypred = model(xtr.reshape(batchSize, 3, 224, 224))\n",
    "            loss = criterian(ypred, ytr.reshape(-1,1))\n",
    "            vls = vls + loss \n",
    "            test.set_description(f\"\\tValidation: {c+1}/{vsteps} steps || Loss: {vls/(c+1):.4f} || Progress\", refresh=True)\n",
    "\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3002]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.5245]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2799]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4702]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2703]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3770]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3524]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2999]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3117]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3554]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3708]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3646]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3198]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3661]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3727]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3319]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3698]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2834]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.5342]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.4750]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2954]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3216]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3506]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.5109]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.5500]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2972]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3292]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.4054]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3673]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3743]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2856]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.5352]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3627]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4696]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4252]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3003]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2948]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3235]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3673]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.5246]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2955]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3178]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2969]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3124]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.5310]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3744]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.5032]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2853]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.5119]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2993]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3011]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3689]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3631]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3004]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.4142]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2869]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.4419]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.5450]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3712]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.5316]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.4064]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3802]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4689]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.5749]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2894]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2768]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3696]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2978]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2810]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.5231]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3629]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.2918]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3753]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4460]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3965]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3732]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3797]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3431]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3654]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4764]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4105]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3714]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3615]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3459]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3303]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.4626]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3838]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2984]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3617]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3670]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3028]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4836]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.3005]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4008]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2837]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.2784]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.4138]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.4251]], grad_fn=<SigmoidBackward0>) tensor(0.)\n",
      "tensor([[0.3622]], grad_fn=<SigmoidBackward0>) tensor(1.)\n",
      "tensor([[0.6012]], grad_fn=<SigmoidBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "for c in range(100, 200):\n",
    "    print(model(X[c].reshape(-1,3,224,224)), Y[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cabed6552182076907bfdc495182d8bb0133da97d0d21fa33aa63cdbe2263e8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
